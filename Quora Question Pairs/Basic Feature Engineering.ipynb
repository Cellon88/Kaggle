{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2345796 entries, 0 to 2345795\n",
      "Data columns (total 3 columns):\n",
      "test_id      int64\n",
      "question1    object\n",
      "question2    object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 53.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('input/train.csv')\n",
    "df_test = pd.read_csv(\"input/test.csv\", low_memory=False, iterator=True, chunksize=600000)\n",
    "df_test = pd.concat(df_test, ignore_index=True)\n",
    "df_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning\n",
    "- **parallelize_dataframe** : Split dataframe into number of cores (4)\n",
    "- **substitute_thousands** : 10k -> 10000\n",
    "- **eng_pos_tagger** : Which -> Which/JJ\n",
    "- **text_cleaning** : remove stop words, stemming words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "from multiprocessing import Pool\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('maxent_treebank_pos_tagger')  \n",
    "num_cores = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parallelize_dataframe(df, func):\n",
    "    df_split = np.array_split(df, num_cores)\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def substitute_thousands(text):\n",
    "    matches = re.finditer(r'[0-9]+(?P<thousands>\\s{0,2}k\\b)', text, flags=re.I)\n",
    "    result = ''\n",
    "    len_offset = 0\n",
    "    for match in matches:\n",
    "        result += '{}000'.format(text[len(result)-len_offset:match.start('thousands')])\n",
    "        len_offset += 3 - (match.end('thousands') - match.start('thousands'))\n",
    "    result += text[len(result)-len_offset:]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "another 15000 keys\n",
      "more 60000 keys\n",
      "a 20 keys\n",
      "10000.\n"
     ]
    }
   ],
   "source": [
    "tests = ['another 15K keys', 'more 60 k keys', 'a 20 keys', '10K.']\n",
    "for test in tests:\n",
    "    print(substitute_thousands(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eng_pos_tagger(text):\n",
    "    # Using averaged_perceptron_tagger, maxent_treebank_pos_tagger\n",
    "    # nltk.download('averaged_perceptron_tagger')\n",
    "    tagger = [\"/\".join(i) for i in pos_tag(text.split())]\n",
    "    return ' '.join(tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Which/JJ fish/NN would/MD survive/VB in/IN salt/NN water?/NN'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tests = \"Which fish would survive in salt water?\"\n",
    "eng_pos_tagger(tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_cleaning(text, remove_stop_words=True, stem_words=True):\n",
    "    # Clean the text with the option to remove stop_words and to stem words.\n",
    "    # Clean the text\n",
    "    text = text.lower()\n",
    "    text = substitute_thousands(text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\" m \", \" am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e-mail\", \"email\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = re.sub(r\"quikly\", \"quickly\", text)\n",
    "    text = re.sub(r\" usa \", \" america \", text)\n",
    "    text = re.sub(r\" u s \", \" america \", text)\n",
    "    text = re.sub(r\"the us\", \"america\", text)\n",
    "    text = re.sub(r\" uk \", \" england \", text)\n",
    "    text = re.sub(r\"imrovement\", \"improvement\", text)\n",
    "    text = re.sub(r\"intially\", \"initially\", text)\n",
    "    text = re.sub(r\" dms \", \"direct messages \", text)  \n",
    "    text = re.sub(r\"demonitization\", \"demonetization\", text) \n",
    "    text = re.sub(r\"actived\", \"active\", text)\n",
    "    text = re.sub(r\"kms\", \" kilometers \", text)\n",
    "    text = re.sub(r\" cs \", \" computer science \", text) \n",
    "    text = re.sub(r\" upvotes \", \" up votes \", text)\n",
    "    text = re.sub(r\"\\0rs \", \" rs \", text)\n",
    "    text = re.sub(r\"calender\", \"calendar\", text)\n",
    "    text = re.sub(r\"programing\", \"programming\", text)\n",
    "    text = re.sub(r\"bestfriend\", \"best friend\", text)\n",
    "    text = re.sub(r\" j k \", \" jk \", text)\n",
    "    \n",
    "    # Remove punctuation from text\n",
    "    text = ''.join([c for c in text if c not in punctuation])\n",
    "    \n",
    "    # Optionally, remove stop words\n",
    "    if remove_stop_words:\n",
    "        text = text.split()\n",
    "        text = [w for w in text if not w in stopwords.words('english')]\n",
    "        text = \" \".join(text)\n",
    "    \n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    \n",
    "    # Return a list of words\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessing(df):\n",
    "    question1 = df['question1'].apply(lambda x: text_cleaning(str(x)))\n",
    "    question2 = df['question2'].apply(lambda x: text_cleaning(str(x)))\n",
    "    return pd.DataFrame({'question1': question1, 'question2': question2, 'is_duplicate': df['is_duplicate']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessing_test(df):\n",
    "    question1 = df['question1'].apply(lambda x: text_cleaning(str(x)))\n",
    "    question2 = df['question2'].apply(lambda x: text_cleaning(str(x)))\n",
    "    return pd.DataFrame({'question1': question1, 'question2': question2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 420 ms, sys: 252 ms, total: 672 ms\n",
      "Wall time: 5min 10s\n"
     ]
    }
   ],
   "source": [
    "%time train = parallelize_dataframe(df, preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             step step guid invest share market india\n",
       "1                      stori kohinoor kohinoor diamond\n",
       "2               increas speed internet connect use vpn\n",
       "3                                     mental lone solv\n",
       "4    one dissolv water quick sugar salt methan carb...\n",
       "5    astrolog capricorn sun cap moon cap risingwhat...\n",
       "6                                            buy tiago\n",
       "7                                       good geologist\n",
       "8                                          use instead\n",
       "9      motorola compani hack charter motorolla dcx3400\n",
       "Name: question1, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.question1[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference in Question length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_match_share(row):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    for word in str(row['question1']).lower().split():\n",
    "        if word not in stops:\n",
    "            q1words[word] = 1\n",
    "    for word in str(row['question2']).lower().split():\n",
    "        if word not in stops:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        return 0\n",
    "    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n",
    "    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n",
    "    R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def median_normalizer(feature):\n",
    "    median = feature.median()\n",
    "    feature = feature.apply(lambda x: (x-median)/np.sqrt(feature.sum()))\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['len_q1'] = df['question1'].apply(lambda x: len(str(x)))\n",
    "df['len_q2'] = df['question2'].apply(lambda x: len(str(x)))\n",
    "df['len_word_q1'] = df['question1'].apply(lambda x: len(str(x).split()))\n",
    "df['len_word_q2'] = df['question2'].apply(lambda x: len(str(x).split()))\n",
    "df['len_word_q2'] = df['len_word_q2'].fillna(0)\n",
    "\n",
    "df['avg_world_len1'] = df['len_q1'] / df['len_word_q1']\n",
    "df['avg_world_len2'] = df['len_q2'] / df['len_word_q2']\n",
    "\n",
    "train['diff_avg_word'] = df['avg_world_len1'] - df['avg_world_len2']\n",
    "train['word_match'] = df.apply(word_match_share, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question to TfidfVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_questions = pd.concat([train['question1'], train['question2']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.22 s, sys: 24 ms, total: 6.25 s\n",
      "Wall time: 6.25 s\n"
     ]
    }
   ],
   "source": [
    "%time tfidf = TfidfVectorizer(lowercase=True, binary=True).fit(all_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q1_tfidf1 = tfidf.transform(train['question1'])\n",
    "q2_tfidf1 = tfidf.transform(train['question2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.4 s, sys: 448 ms, total: 28.9 s\n",
      "Wall time: 28.9 s\n"
     ]
    }
   ],
   "source": [
    "%time tfidf = TfidfVectorizer(lowercase=True, binary=True, ngram_range=(1,3), analyzer='word', \\\n",
    "                              max_features=100000, max_df=0.5, min_df=30, use_idf=True).fit(all_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q1_tfidf2 = tfidf.transform(train['question1'])\n",
    "q2_tfidf2 = tfidf.transform(train['question2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 49s, sys: 4.74 s, total: 4min 54s\n",
      "Wall time: 4min 54s\n"
     ]
    }
   ],
   "source": [
    "%time count = CountVectorizer(lowercase=True, binary=True, ngram_range=(1,10), analyzer='char', \\\n",
    "                              max_features=300000, max_df=0.999, min_df=50).fit(all_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q1_count = count.transform(train['question1'])\n",
    "q2_count = count.transform(train['question2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Distance, Similarity\n",
    "- paired cosine distances\n",
    "- pairwise distances (cosine, euclidean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import paired_cosine_distances\n",
    "from nltk.metrics.distance import jaccard_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.9 s, sys: 1.16 s, total: 22.1 s\n",
      "Wall time: 22.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train['tf_distance1'] = paired_cosine_distances(q1_tfidf1, q2_tfidf1)\n",
    "train['tf_distance2'] = paired_cosine_distances(q1_tfidf2, q2_tfidf2)\n",
    "train['cnt_distance'] = paired_cosine_distances(q1_count, q2_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['tf_cosine1'] = pairwise_distances(q1_tfidf1, q2_tfidf1, metric='cosine', n_jobs=-1)\n",
    "train['tf_cosine2'] = pairwise_distances(q1_tfidf2, q2_tfidf2, metric='cosine', n_jobs=-1)\n",
    "train['cnt_cosine'] = pairwise_distances(q1_count, q2_count, metric='cosine', n_jobs=-1)\n",
    "train['tf_eucdian1'] = pairwise_distances(q1_tfidf1, q2_tfidf1, metric='euclidean', n_jobs=-1)\n",
    "train['tf_eucdian2'] = pairwise_distances(q1_tfidf2, q2_tfidf2, metric='euclidean', n_jobs=-1)\n",
    "train['cnt_eucdian'] = pairwise_distances(q1_count, q2_count, metric='euclidean', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['jaccard_dist'] = df.apply(\n",
    "    lambda x: jaccard_distance(set(str(x.question1).split(' ')), set(str(x.question2).split(' '))), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.to_pickle('input/train.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 404290 entries, 0 to 404289\n",
      "Data columns (total 9 columns):\n",
      "is_duplicate     404290 non-null int64\n",
      "question1        404290 non-null object\n",
      "question2        404290 non-null object\n",
      "diff_avg_word    404290 non-null float64\n",
      "tf_distance1     404290 non-null float64\n",
      "tf_distance2     404290 non-null float64\n",
      "cnt_distance     404290 non-null float64\n",
      "jaccard_dist     404290 non-null float64\n",
      "word_match       404290 non-null float64\n",
      "dtypes: float64(6), int64(1), object(2)\n",
      "memory usage: 30.8+ MB\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_pickle('input/train.p')\n",
    "train.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
