{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2345796 entries, 0 to 2345795\n",
      "Data columns (total 3 columns):\n",
      "test_id      int64\n",
      "question1    object\n",
      "question2    object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 53.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('input/train.csv')\n",
    "df_test = pd.read_csv(\"input/test.csv\", low_memory=False, iterator=True, chunksize=600000)\n",
    "df_test = pd.concat(df_test, ignore_index=True)\n",
    "df_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning\n",
    "- **parallelize_dataframe** : Split dataframe into number of cores (4)\n",
    "- **substitute_thousands** : 10k -> 10000\n",
    "- **eng_pos_tagger** : Which -> Which/JJ\n",
    "- **text_cleaning** : remove stop words, stemming words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "from multiprocessing import Pool\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('maxent_treebank_pos_tagger')  \n",
    "num_cores = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parallelize_dataframe(df, func):\n",
    "    df_split = np.array_split(df, num_cores)\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def substitute_thousands(text):\n",
    "    matches = re.finditer(r'[0-9]+(?P<thousands>\\s{0,2}k\\b)', text, flags=re.I)\n",
    "    result = ''\n",
    "    len_offset = 0\n",
    "    for match in matches:\n",
    "        result += '{}000'.format(text[len(result)-len_offset:match.start('thousands')])\n",
    "        len_offset += 3 - (match.end('thousands') - match.start('thousands'))\n",
    "    result += text[len(result)-len_offset:]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "another 15000 keys\n",
      "more 60000 keys\n",
      "a 20 keys\n",
      "10000.\n"
     ]
    }
   ],
   "source": [
    "tests = ['another 15K keys', 'more 60 k keys', 'a 20 keys', '10K.']\n",
    "for test in tests:\n",
    "    print(substitute_thousands(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eng_pos_tagger(text):\n",
    "    # Using averaged_perceptron_tagger, maxent_treebank_pos_tagger\n",
    "    # nltk.download('averaged_perceptron_tagger')\n",
    "    tagger = [\"/\".join(i) for i in pos_tag(text.split())]\n",
    "    return ' '.join(tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Which/JJ fish/NN would/MD survive/VB in/IN salt/NN water?/NN'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tests = \"Which fish would survive in salt water?\"\n",
    "eng_pos_tagger(tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_cleaning(text, remove_stop_words=True, stem_words=True):\n",
    "    # Clean the text with the option to remove stop_words and to stem words.\n",
    "    # Clean the text\n",
    "    text = text.lower()\n",
    "    text = substitute_thousands(text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\" m \", \" am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e-mail\", \"email\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = re.sub(r\"quikly\", \"quickly\", text)\n",
    "    text = re.sub(r\" usa \", \" america \", text)\n",
    "    text = re.sub(r\" u s \", \" america \", text)\n",
    "    text = re.sub(r\"the us\", \"america\", text)\n",
    "    text = re.sub(r\" uk \", \" england \", text)\n",
    "    text = re.sub(r\"imrovement\", \"improvement\", text)\n",
    "    text = re.sub(r\"intially\", \"initially\", text)\n",
    "    text = re.sub(r\" dms \", \"direct messages \", text)  \n",
    "    text = re.sub(r\"demonitization\", \"demonetization\", text) \n",
    "    text = re.sub(r\"actived\", \"active\", text)\n",
    "    text = re.sub(r\"kms\", \" kilometers \", text)\n",
    "    text = re.sub(r\" cs \", \" computer science \", text) \n",
    "    text = re.sub(r\" upvotes \", \" up votes \", text)\n",
    "    text = re.sub(r\"\\0rs \", \" rs \", text)\n",
    "    text = re.sub(r\"calender\", \"calendar\", text)\n",
    "    text = re.sub(r\"programing\", \"programming\", text)\n",
    "    text = re.sub(r\"bestfriend\", \"best friend\", text)\n",
    "    text = re.sub(r\" j k \", \" jk \", text)\n",
    "    \n",
    "    # Remove punctuation from text\n",
    "    text = ''.join([c for c in text if c not in punctuation])\n",
    "    \n",
    "    # Optionally, remove stop words\n",
    "    if remove_stop_words:\n",
    "        text = text.split()\n",
    "        text = [w for w in text if not w in stopwords.words('english')]\n",
    "        text = \" \".join(text)\n",
    "    \n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    \n",
    "    # Return a list of words\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessing(df):\n",
    "    question1 = df['question1'].apply(lambda x: text_cleaning(str(x)))\n",
    "    question2 = df['question2'].apply(lambda x: text_cleaning(str(x)))\n",
    "    return pd.DataFrame({'question1': question1, 'question2': question2, 'is_duplicate': df['is_duplicate']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 512 ms, sys: 196 ms, total: 708 ms\n",
      "Wall time: 5min 12s\n"
     ]
    }
   ],
   "source": [
    "%time train = parallelize_dataframe(df, preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             step step guid invest share market india\n",
       "1                      stori kohinoor kohinoor diamond\n",
       "2               increas speed internet connect use vpn\n",
       "3                                     mental lone solv\n",
       "4    one dissolv water quick sugar salt methan carb...\n",
       "5    astrolog capricorn sun cap moon cap risingwhat...\n",
       "6                                            buy tiago\n",
       "7                                       good geologist\n",
       "8                                          use instead\n",
       "9      motorola compani hack charter motorolla dcx3400\n",
       "Name: question1, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.question1[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference in Question length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['len_q1'] = df['question1'].apply(lambda x: len(str(x)))\n",
    "df['len_q2'] = df['question2'].apply(lambda x: len(str(x)))\n",
    "\n",
    "df['len_word_q1'] = df['question1'].apply(lambda x: len(str(x).split()))\n",
    "df['len_word_q2'] = df['question2'].apply(lambda x: len(str(x).split()))\n",
    "df['len_word_q2'] = df['len_word_q2'].fillna(0)\n",
    "\n",
    "df['avg_world_len1'] = df['len_q1'] / df['len_word_q1']\n",
    "df['avg_world_len2'] = df['len_q2'] / df['len_word_q2']\n",
    "train['diff_avg_word'] = df['avg_world_len1'] - df['avg_world_len2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def median_normalizer(feature):\n",
    "    median = feature.median()\n",
    "    feature = feature.apply(lambda x: (x-median)/np.sqrt(feature.sum()))\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question to TfidfVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_questions = train['question1'].append(train['question2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.29 s, sys: 20 ms, total: 6.31 s\n",
      "Wall time: 6.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(lowercase=True, binary=True)\n",
    "%time tfidf.fit(all_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q1_tfidf1 = tfidf.transform(train['question1'])\n",
    "q2_tfidf1 = tfidf.transform(train['question2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.4 s, sys: 328 ms, total: 28.7 s\n",
      "Wall time: 28.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.5, max_features=100000, min_df=30,\n",
       "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(lowercase=True, binary=True, ngram_range=(1,3), analyzer='word',\n",
    "                        max_features=100000, max_df=0.5, min_df=30, use_idf=True)\n",
    "%time tfidf.fit(all_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q1_tfidf2 = tfidf.transform(train['question1'])\n",
    "q2_tfidf2 = tfidf.transform(train['question2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 49s, sys: 4.8 s, total: 4min 54s\n",
      "Wall time: 4min 53s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='char', binary=True, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.999, max_features=300000, min_df=50,\n",
       "        ngram_range=(1, 10), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = CountVectorizer(lowercase=True, binary=True, ngram_range=(1,10), analyzer='char', \n",
    "                        max_features=300000, max_df=0.999, min_df=50)\n",
    "%time count.fit(all_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q1_count = count.transform(train['question1'])\n",
    "q2_count = count.transform(train['question2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction with LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=150, random_state=42)\n",
    "lsa = make_pipeline(svd, Normalizer(norm='l2', copy=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_vect = df['q1_vect'].append(df['q2_vect'], ignore_index=True)\n",
    "%time lsa.fit_transform(np.asarray(all_vect))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Distance, Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import paired_cosine_distances\n",
    "from nltk.metrics.distance import jaccard_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 308 ms, sys: 28 ms, total: 336 ms\n",
      "Wall time: 337 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train['tf_distance1'] = paired_cosine_distances(q1_tfidf1, q2_tfidf1)\n",
    "train['tf_distance2'] = paired_cosine_distances(q1_tfidf2, q2_tfidf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.9 s, sys: 512 ms, total: 17.4 s\n",
      "Wall time: 17.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train['cnt_similarity'] = -(q1_count != q2_count).astype(int)\n",
    "train['jaccard_dist'] = df.apply(lambda x: jaccard_distance(set(str(x.question1).split(' ')), \n",
    "                                                            set(str(x.question2).split(' '))), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.to_pickle('input/train.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 404290 entries, 0 to 404289\n",
      "Data columns (total 8 columns):\n",
      "is_duplicate      404290 non-null int64\n",
      "question1         404290 non-null object\n",
      "question2         404290 non-null object\n",
      "diff_avg_word     404290 non-null float64\n",
      "tf_distance1      404290 non-null float64\n",
      "tf_distance2      404290 non-null float64\n",
      "cnt_similarity    404290 non-null object\n",
      "jaccard_dist      404290 non-null float64\n",
      "dtypes: float64(4), int64(1), object(3)\n",
      "memory usage: 27.8+ MB\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_pickle('input/train.p')\n",
    "train.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
